# A script to calculate emissions changes associated with changes to the fossil
#   fuel load in a region. Depends on objects generated by avertr_setup.R. Test
#   outputs from avertr against outputs from AVERT using avertr_test.R/



# START TIME ######
# For tracking runtime
start_time <- Sys.time()



# SET UP ########
rm(list = ls())

library(tidyverse)
library(readxl)



# USER INPUT ########

# NEEDS WORK
project_capacity <- 500

project_region <- "new-england"

project_type <- "offshore_wind"


# DEFINE/LOAD OBJECTS ######
# Hourly capacity factor; remove the 24 hours corresponding to 2/29
capacity_factor_8760 <- read_excel(
  "./avert-main-module-v4.3.xls",
  sheet = "EERE_Default",
  range = "AJ2:AJ8786"
) |> 
  #slice(c(1:1416, 1441:n())) |> 
  slice(c(1:8760)) |> # NOTE!!! This is the wrong slice, but it's the one AVERT
  #   uses. The above, commented-out slice removes the correct dates.
  #   However, using what AVERT does for now for validation.
  pull(`Offshore Wind`)

# This is simply a vector of each hour of the year 2023
datetime_8760 <- seq(
  from = ymd_hms("2023-01-01 00:00:00"),
  by = "1 hour",
  length.out = 8760
)

# NEI emission factors (cleaned in avertr_setup)
# EVENTUALLY: save 14 versions, only load one
nei_efs <- read_rds("./avertr_setup_output/nei_efs.rds")

# EVENTUALLY: only loading in for the specific region
ff_load_bin_data_final_region <- read_rds("./avertr_setup_output/ff_load_bin_data_final.rds")[[7]]

# The BAU scenario results
bau_scenario_region <- read_rds("./bau_scenarios/new-england_bau_scenario_2023.rds")

# This is the BAU load
bau_load_8760 <- bau_scenario_region |> 
  arrange(datetime_8760_col) |> 
  distinct(datetime_8760_col, load_8760_col) |> 
  pull(load_8760_col)

# This is the hourly capacity
capacity_8760 <- capacity_factor_8760 * project_capacity

# This is the new hourly net load
new_load_8760 <- bau_load_8760 - capacity_8760

# This is a vector containing the set of ff load bins for the region
ff_load_bin_key <- ff_load_bin_data_final_region |>
  distinct(ff_load_bin_col) |> 
  pull(ff_load_bin_col)


# BINNIFY #######
# Based on the project, find the new ff load bins

# NA vector for now, but will eventually store the "binnified" version of each
#   vector from load_8760s_bau
ff_load_bin_8760 <- rep(NA, 8760)

# This function binnifies the vector. Specifically, for each raw hourly load
#   within load_8760s_bau, it finds the closest load bin which is less than or
#   equal to the element. It does not simply find the closest load bin —
#   it never matches to a higher load bin. This is because of the way that we
#   do the interpolation below.
binnify <- function(load_8760, ff_load_bin_key) {
  
  # For each hour of the year
  for (i in 1:8760) {
    
    # Gives a vector containing the difference between raw load and each load
    #   bin within the region.
    diff <- load_8760[i] - ff_load_bin_key
    
    # Exclude all bins where the load bin exceeds the raw load
    diff[diff < 0] <- NA
    
    # Find the smallest difference
    bin_index <- which.min(diff)
    
    # The load bin with the smallest difference is our load bin in this slot
    ff_load_bin_8760[i] <- ff_load_bin_key[bin_index]
  }
  return(ff_load_bin_8760)
}

ff_load_bin_8760 <- binnify(new_load_8760, ff_load_bin_key)

# Make the binned load into a tibble and bind it with a column for date time
#   and a column for the raw load.
ff_load_bin_8760 <- ff_load_bin_8760 |> 
  as_tibble_col(column_name = "ff_load_bin_8760_col") |> 
  bind_cols(
    datetime_8760_col = datetime_8760,
    load_8760_col = new_load_8760,
  ) |> 
  relocate(ff_load_bin_8760_col, .after = load_8760_col)


# ASSIGN DATA VALUES ##########
# Now we join the data measures onto the appropriate load bins.
assigned_data <- ff_load_bin_8760 |> 
  left_join(
    ff_load_bin_data_final_region,
    by = join_by(ff_load_bin_8760_col == ff_load_bin_col)
  )

# # Check: there should be the same number of rows as in the bau scenario tibble
# nrow(ff_load_bin_8760) == nrow(bau_scenario_region)



# OZONE SEASON SPLIT =============
# In each region, for each hour of the year, we have a data measure giving ozone
#   season values and a data measure giving non-ozone season values (besides
#   generation, which does not depend on the ozone season). But we only want the
#   ozone season values for hours in the ozone season, and the non-ozone season
#   values for values not in the ozone season.

ozone_split <- function(assigned_data) {
  
  # These are the rows in the ozone season
  oz <- assigned_data |> 
    filter(
      between(
        datetime_8760_col,
        ymd_hms("2023-05-01 00:00:00"),
        ymd_hms("2023-09-30 23:00:00")
      )
    )
  
  # These are the rows not in the ozone season
  non <- assigned_data |> 
    anti_join(oz, by = join_by(datetime_8760_col))
  
  # For the rows in the ozone season, deselect the non-ozone season data
  oz <- oz |> 
    select(datetime_8760_col:data_next_bin_generation, contains("ozone")) |> 
    # Rename so that we can easily bind rows below
    rename_with(~ str_replace(., "_ozone", ""))
  
  # For the rows in the non-ozone season, deselect the ozone season data
  non <- non |> 
    select(datetime_8760_col:data_next_bin_generation, contains("non")) |> 
    # Rename so that we can easily bind rows belo
    rename_with(~ str_replace(., "_non", ""))
  
  assigned_data_ozoned <- bind_rows(oz, non)
  
  return(assigned_data_ozoned)
}

assigned_data_selected <- ozone_split(assigned_data)



# INTERPOLATE ##########
# Now we interpolate the data. For each hour of the year, for each EGU, we
#   have its data measure for the load bin and the next load bin. Recall that
#   the raw generation value (for the EGU, for the hour) falls between the load
#   bin and the next load bin. Thus, based on the raw generation value, we
#   linearly interpolate between the two load bins.

interpolate <- function(assigned_data) {
  ff_load_bin_8760_bau <- pull(assigned_data, ff_load_bin_8760_col)
  
  ff_load_bin_8760_next_bau <- pull(assigned_data, ff_load_bin_next_col)
  
  load_8760_bau <- pull(assigned_data, load_8760_col)
  
  # The "metadata" here is the hour datetime, the raw load, and the load bin
  metadata <- select(assigned_data, datetime_8760_col:ff_load_bin_next_col)
  
  current_data <- assigned_data |>
    select(contains("data") & !contains("next"))
  
  next_data <- assigned_data |>
    select(contains("data") & contains("next"))
  
  # This process comes directly from the AVERT macros code
  interpolate_inner <- function(cd, nd) {
    slope <- (cd - nd) / (ff_load_bin_8760_bau - ff_load_bin_8760_next_bau)
    intercept <- cd - (slope * ff_load_bin_8760_bau)
    val <- (load_8760_bau * slope) + intercept
    return(val)
  }
  
  # modify2 used because it returns a tibble (map2 would return a list)
  interped_inner <- modify2(current_data, next_data, interpolate_inner)
  
  interped_inner <- bind_cols(metadata, interped_inner)
  
  return(interped_inner)
  
}

interped_data_regions <- interpolate(assigned_data_selected)



# ADD PM2.5, VOCs, NH3 #######



# GET DIFFERENCES #######














join_data <- function(x_ff_load_bins, colnm) {
  colnm_sym <- sym(colnm)
  x_ff_load_bins <- x_ff_load_bins |> 
    pivot_longer(
      any_of(as.character(ff_load_bins_key)),
      names_to = "ff_load_bins_8760",
      values_to = colnm) |> 
    mutate(
      ff_load_bins_8760 = as.numeric(ff_load_bins_8760)
      # nextnum = lead(!!colnm_sym),
      # nextnum = case_when( # When the load bin is as high as it can go, there's no "next" load bin. lead() pulls in the lowest load bin, but it shouldn't
      #   ff_load_bins_8760 == max(ff_load_bins_key) ~ NA,
      #   TRUE ~ nextnum
      #)
    )
  return(x_ff_load_bins) # This returns a df which has a row for each unit for each load bin.
}

# Get all the EGU names
EGU_names_key <- generation_ff_load_bins |> 
  pull(`Full Unit Name`)

# In each df, select only relevant cols
ff_load_bins_list <- ff_load_bins_list |> 
  map(~select(., !(State:`Unit Code`)))

# Apply the above tidying function
ff_load_bins_list <- ff_load_bins_list |> 
  map2(names(ff_load_bins_list), ~join_data(.x, .y))


## BAU Scenario =========
# This is the information for the BAU scenario — the one where there's no load
#   difference

# Take Cartesian product of ff_load_bins and EGU names. So we end up with
#    a row for each EGU name-load bin pair
test_bau <- expand.grid(
  ff_load_bins_8760_bau = ff_load_bins_8760_bau, # Change this ot have a name analogous to ff_load_bins, etc. OR maybe just don't change name at all?
  `Full Unit Name` = EGU_names_key
) |> 
  as_tibble() |> 
  add_column(datetime = rep(datetime_8760, length(EGU_names_key)),
             ff_load_bins_8760_bau_next = rep(ff_load_bins_8760_bau_next, length(EGU_names_key)),
             bau_load_hourly_mw = rep(bau_load_hourly_mw, length(EGU_names_key)))



# Eventually: might be best to save the ff load bin index directly, rather than
#   calculate load bin so early on
# Use the load bin col and the ff_load_bins_key to find the next load bin.
# Do the join twice: once for the current load bin, and once for the next
#   load bin. (What if you're in the max load bin? Shouldn't happen. You only get
#   into a load bin if true load is above it, and if true load is above the max
#   load bin, it shouldn't run.)
# Take the difference in the two values across the load bins, divide by the difference
#   in the two load bins. That's slope.
# Backfit for intercept
#   Calculate value.

# FOR NOW you're not worry abt cases where you have a load bin as max load, but
#   eventually you need to add that functionality, since someone could run a 
#   scenario like that.

# This is the biggest slowdown w the current algo: you join twice, even though
#   after the first join you can easily determine what the second join should be,
#   simply based on the matrix location of the values. Maybe best way is to only
#   load in index values in the above for loop, and then just literally index each
#   table in the ff_load_bins_list?
for (i in 1:length(ff_load_bins_list)) {
  test_bau <- test_bau |> 
    left_join(ff_load_bins_list[[i]], by = join_by(ff_load_bins_8760_bau == ff_load_bins_8760, `Full Unit Name`)) |> 
    left_join(ff_load_bins_list[[i]], by = join_by(ff_load_bins_8760_bau_next == ff_load_bins_8760, `Full Unit Name`))
}



# Rename columns
test_bau <- test_bau |>
  rename_with(
    .fn = ~str_remove(., "_ff_load_bins"),
    .cols = !c(ff_load_bins_8760_bau:`Full Unit Name`)
  )



## Alt Scenario =========
# Take Cartesian product of ff_load_bins and EGU names. So we end up with
#    a row for each EGU name-load bin pair
test <- expand.grid(
  ff_load_bins_8760 = ff_load_bins_8760,
  `Full Unit Name` = EGU_names_key
) |> 
  as_tibble() |> 
  add_column(datetime = rep(datetime_8760, length(EGU_names_key)),
             ff_load_bins_8760_next = rep(ff_load_bins_8760_next, length(EGU_names_key)),
             new_load_hourly_mw = rep(new_load_hourly_mw, length(EGU_names_key)))






# join on relevant values
for (i in 1:length(ff_load_bins_list)) {
  test <- test |> 
    left_join(ff_load_bins_list[[i]], by = join_by(ff_load_bins_8760, `Full Unit Name`)) |> 
    left_join(ff_load_bins_list[[i]], by = join_by(ff_load_bins_8760_next == ff_load_bins_8760, `Full Unit Name`))
}





# Rename columns
test <- test |>
  rename_with(
    .fn = ~str_remove(., "_ff_load_bins"),
    .cols = !c(ff_load_bins_8760:`Full Unit Name`)
  )

# OZONE SEASON SPLIT #############
# Split up into ozone season and not. As per AVERT documentation, ozone season is
#   May to September inclusive.
test_ozone <- test |>
  filter(between(datetime, ymd_hms("2012-05-01 00:00:00"), ymd_hms("2012-09-30 23:00:00")))

test_non <- test |>
  anti_join(test_ozone, by = join_by(datetime))

# # Check:
# nrow(test_ozone) + nrow(test_non) == nrow(test)
# intersect(test_ozone$datetime, test_non$datetime)

test_ozone <- test_ozone |> 
  select(ff_load_bins_8760:generation.y, contains("ozone")) |> 
  rename_with(~str_replace(., "_ozone", ""))

test_non <- test_non |> 
  select(ff_load_bins_8760:generation.y, !contains("ozone")) |> 
  rename_with(~str_replace(., "_non", ""))

test <- test_ozone |> 
  bind_rows(test_non)

# end.time <- Sys.time()
# time.taken <- end.time - start.time
  








test_bau_ozone <- test_bau |>
  filter(between(datetime, ymd_hms("2012-05-01 00:00:00"), ymd_hms("2012-09-30 23:00:00")))

test_bau_non <- test_bau |>
  anti_join(test_bau_ozone, by = join_by(datetime))

# # Check:
# nrow(test_bau_ozone) + nrow(test_bau_non) == nrow(test_bau)
# intersect(test_bau_ozone$datetime, test_bau_non$datetime)

test_bau_ozone <- test_bau_ozone |> 
  select(ff_load_bins_8760_bau:generation.y, contains("ozone")) |> 
  rename_with(~str_replace(., "_ozone", ""))

test_bau_non <- test_bau_non |> 
  select(ff_load_bins_8760_bau:generation.y, !contains("ozone")) |> 
  rename_with(~str_replace(., "_non", ""))

test_bau <- test_bau_ozone |> 
  bind_rows(test_bau_non)


# INTERPOLATION #########
get_val_bau <- function(current_bin, next_bin) {
  slope <- (current_bin - next_bin) / (test_bau$ff_load_bins_8760_bau - test_bau$ff_load_bins_8760_bau_next)
  intercept <- current_bin - (slope * test_bau$ff_load_bins_8760_bau)
  val <- (test_bau$bau_load_hourly_mw * slope) + intercept
  return(val)
}

current_load_bin_vals_bau <- test_bau |> 
  select(contains(".x")) |> 
  as.list()

next_load_bin_vals_bau <- test_bau |> 
  select(contains(".y")) |> 
  as.list()

res_list_bau <- map2(current_load_bin_vals_bau, next_load_bin_vals_bau, get_val_bau)

res_temp_bau <- res_list_bau |>
  c(select(test_bau, `Full Unit Name`)) |> 
  as_tibble() |> 
  left_join(nei_efs, by = join_by(`Full Unit Name` == `Full Name`))

res_temp_bau <- res_temp_bau |> 
  mutate(pm25.x = PM2.5...36 * heat.x,
         vocs.x = VOCs...37 * heat.x,
         nh3.x = NH3...38 * heat.x)

res_list_bau <- res_temp_bau |> 
  select(!c(`Full Unit Name`, PM2.5...36:NH3...38)) |> 
  as.list()



# CHECK AVERT CODE for how they do this...






get_val <- function(current_bin, next_bin) { # EVENTUALLY: seems best to just have one get_val function, with three additional arguments for the three specific cols, OR you can name the cols the same across test and test_bau, and just pass one additional argument for the df...
  slope <- (current_bin - next_bin) / (test$ff_load_bins_8760 - test$ff_load_bins_8760_next)
  intercept <- current_bin - (slope * test$ff_load_bins_8760)
  val <- (test$new_load_hourly_mw * slope) + intercept
  return(val)
}

current_load_bin_vals <- test |> 
  select(contains(".x")) |> 
  as.list()

next_load_bin_vals <- test |> 
  select(contains(".y")) |> 
  as.list()

res_list <- map2(current_load_bin_vals, next_load_bin_vals, get_val)

res_temp <- res_list |>
  c(select(test, `Full Unit Name`)) |> 
  as_tibble() |> 
  left_join(nei_efs, by = join_by(`Full Unit Name` == `Full Name`))

res_temp <- res_temp |> 
  mutate(pm25.x = PM2.5...36 * heat.x,
         vocs.x = VOCs...37 * heat.x,
         nh3.x = NH3...38 * heat.x)

res_list <- res_temp |> 
  select(!c(`Full Unit Name`, PM2.5...36:NH3...38)) |> 
  as.list()



qt <- read_rds("bau_scenarios_2023.rds")

set.seed(50)
qt[[7]] |> arrange(data_generation, data_so2, data_nox) |> sample_n(100) |> view()

set.seed(50)
res_list_bau |> bind_cols() |> arrange(generation.x, so2.x, nox.x) |> sample_n(100) |> view()

# Eventually: you'll want to split this up into two steps: just find raw
#   differences, then round some to 3, some to 6
changes <- map2(res_list, res_list_bau, function(.x, .y) round(.x - .y, 6))


final_changes <- test |> 
  select(ff_load_bins_8760:new_load_hourly_mw) |> 
  add_column(bind_cols(changes))


tt <- final_changes |> summarize(
  across(generation.x:nh3.x, sum),
  .by = `Full Unit Name`
)






# Later: do a full check to ensure variable names are reasonable in each
#   script, and that they're compatible between scripts





# IDEAS (as of 4/8, 2pm): Looks like they exclude load hours outside of
#   the lowest load bin or highest load bin. (Note: looks like this is
#   checked against the raw load, not  the load binnified version of it)




# Recall that you can open up macros and compile if you need to,
#   add breakpoints, and then hover over variable names to get the values.



# NOTE: you will run up against rare so2 emission events plants (altho
#   not in New England)



